## 大佬观点
### 刘知远--科研中的好想法从哪里来？  


###  因果推理相关笔记  

#### 自己思考  
1. 什么是因果？   
2. 因果 和 深度学习的关系？   因果 和 自然语言处理、自然语言理解、常识推理、专业知识推理的关系？   

3. 我们所说的 因果推理、因果关系，对于  例子:  A= 下雨天  B=打伞  ，我们想让模型知道  下雨天应该打伞  这个事实吗？   还是  下雨天我应该要打伞这个道理。  好像也还是没有说清楚。  

4. 机器怎么学习因果推理？  怎么评测机器的推理能力？   






#### 1. 图灵奖得主Bengio再次警示：可解释因果关系是深度学习发展的当务之急    

* 除非深度学习能够超越模式识别并了解因果关系的更多信息，否则它将无法实现其全部潜力，也不会带来真正的AI革命。换句话说，深度学习需要开始知道事情发生的因果关系。     【注: 是 变量之间的因果关系？ 还是模型决策层面的因果关系？  或者二者怎么联系？ 】 
*  将因果关系集成到 AI 中是一件大事，当前的机器学习方法都假设经过训练的 AI 系统将应用于与训练数据相同类型的数据上。然而，在现实生活中，情况往往并非如此。      【这个例子和因果有什么关系？】
*  使用了一个数据集，以概率的形式绘制了诸如吸烟和肺癌等现实世界现象之间的因果关系。他们还生成了因果关系的综合数据集。    【这算是 事件、变量层面的因果？ 】  
*  深度学习算法并不善于概括，也不善于将它们从一个上下文中学到的东西应用到另一个上下文中。它们能够捕获相关的现象，例如公鸡啼叫和太阳升起，但是无法考虑彼此之间的因果关系。 【深度学习-分布式的问题？  】  
*  

#### 2. Bengio：深度学习不会被取代，我想让 AI 会推理、计划和想象      

#### 3. 深度学习可解释性问题如何解决？图灵奖得主Bengio有一个解  【自己发表的一篇文章】  
* 《通过元迁移目标来学习理解因果关系》一文，提出了一种基于学习器适应稀疏分布变化速度的元学习因果结构，还生成了因果关系的综合数据集。  
* 提出了 一种基于学习器适应稀疏分布 变化  速度的元学习因果结构，这些变化因素比如: 干预、智能体的行为、其他不稳定因素。 





### 王炳宁师兄--机器阅读理解关键技术研究--博士论文    
#### 第一章 绪论
##### 背景和意义  
1.1.1 定义: 使人像机器一样可以阅读文档，并理解文档;  
1.1.2 应用: 
    * 问答系统： 理解无结构化的文本，回答复杂的问题；   
    * 对话系统:  在给定上下文的场景下，系统理解背景文档之后给出相关性更高的会复等；
    * 信息检索： 深层次理解文档的含义，帮助检索出 语义相关度高的文档等； 
1.2 阅读理解: 
1.2.1 阅读
阅读将抽象的符号转换为概念，并对概念进行分析、加工、推理，以产生新的知识; 
阅读包含: 
    * 1) 语音意识：基于字词的音素、音节并将其组合成单词的发音； 
    * 2) 字词理解：字词读音和代表意思结合
    * 3) 关联阅读：将连续的词、字母的意思组合成形成字符串。
    * 4) 阅读理解：将阅读形成的概念与脑海中存在的先验知识进行对比、验证，从而实现理解文本。   
1.2.2 理解  
理解是阅读的目的，也是阅读的心智产物。 
语言角度，理解分为三个类别: 
    * 1) 层次1--字面理解: 指能够直接从文章中提取出显式信息表达的能力，信息: 事实、主题、类别、轮廓、总结; 
    * 2) 层次2--推断理解: 建立在字面理解的基础之上。对信心进行加工处理，并推断出文章中没有显式指出的事实[非字面匹配]。 需要将文章中的内容含义 动态的存储在大脑的记忆单元中，并在这个过程中有意识的搜索线索，是更高层次的理解。 
    * 3) 层次3--评价性理解: 批判性理解、应用理解，主要是读者根据自己的经验和观点对文章进行分析和评价。 不仅需要字面理解和推断理解文本内容，更需要将已理解的内容和脑中的先验知识进行融合，从而对作者的状态、目的、可能产生的结果进行评价。   评价理解由于先验知识不同，所以因人而异。  
  * 总结: 研究表明:人类对于字面理解能力尚可，对推断理解、评价理解表现相对较弱。  
1.2.3 阅读理解 
阅读理解是建立在阅读基础之上的理解，可以被抽象的概括为通过从文本中提取信息并且理解意义的过程。  需要: 1) 低层次语言能力; 2)高层次认知能力： 
  * 自动化: 自动对于字词意思进行解码，如 语音意识、字词理解。  
  * 先验知识和图式构造: 根据当前文本调用以前的先验知识并形成图式，并将当前文本的信息填入相应的图式中获得当前文本的含义。  
  * 文体知识: 对于不同文体有不同的处理过程。 
  * 总结: 
    * 1) 传统认为: 先掌握低层次技能，才具有理解文本能力。 
    * 2) 现代语言学家、教育学家: 低层次的技能和高层次的技能需要同时发挥作用。 
1.2.4 阅读理解的研究: 
随着认知心理学、心理语言学发展，阅读理解从传统对 纯阅读行为研究 --> 信息处理过程; 
认知角度: 1) 认知模式  2)阅读理解的图式理论
阅读理解的认知模式有三种类型:
    * 自底向上模式: 字词辨认--短语、句子意思--更高层次。 
      * 优点:强调 解码-自动化的过程。 
      * 缺点: 1) 信息处理看作pipline,人类并不用所有信息。 
              2) 很难利用 句子和背景知识关系，无法反馈，忽视读者本身在阅读过程中的重要性；  
    * 自顶向下模式: 采样文本信息--和自己知识对比--构建具体意义。
      * 优点: 有选择利用输入，重视作者信息。 
      * 缺点: 过分强调读者重要性，读者对文章主题内容不熟悉时，很难进行判断和推测。2) 在产生预测、判断之后才能对 文章进行深层次理解，并不能体现人类理解过程中的自动化。 
    * 相互作用模式:  理解语言文字--背景知识的运用、推理。  
阅读理解的图式理论: 
图式理论: 图式是认知的基础，是存储于人脑记忆中用于表达一般概念的框架。 阅读文章过程是读者知识和文章内容信息之间双向交流、作用的过程，对应于图式理论中已有信息图式与新信息之间建立联系的过程。 阅读理解过程中，读者的阅读能力和理解程度由三种图式决定: 语言图式、内容图式、形式图式。 
    * 语言图式: 读者的词汇、语法、句型等语言基础知识。 
    * 内容图式: 文章内容的知识，读者对文章主题的熟悉程度。 
    * 形式图式: 文章的文化和语篇结构方面的知识，读者对文章体裁的熟悉程度。 
阅读过程中，大脑中三种图式分别与文章的语言、内容、形式相关作用，进而对文章进行理解。 

1.3 从阅读理解到机器阅读理解  
心理学家、认知科学家定义阅读理解: 人类对文本的理解不仅停留在 词汇、语法、基础语义单元，更是一种深层次、交互式的文本内容理解。  
机器阅读理解: 系统不仅能处理基本的语言学信息，词法、语法、语义，更要完成深层次理解，从字面理解 到 推断理解 甚至是 评价性理解。    
1.3.1 技术模块: 
人的认知能力可以在语言能力上快速发展。 机器没有普遍文法，处理信息的能力要从头构建，所以机器阅读理解，必须赋予机器两方面能力: 1) 阅读(语言)能力  2) 理解(认知)能力; 
1.3.1.1 语言能力: 
人语言能力: 语音意识、字词理解、关联能力  
机器语言能力:     词义辨析 、  句法识别、语义组合  
NLP技术、任务:    词义消歧、词性标注、  句法分析、语义标注  
注意: 人类可以自主应用‘世界知识’，但是系统很难利用‘背景知识’，所以自底向上模式的语言能力是完成机器阅读理解 必不可少的一部分。 

1.3.1.2 认知能力:   
从理解角度，机器阅读理解和一般自然语言处理任务最大区别: 不仅需要 字面理解，更需要推断理解和评价性理解，强调认知能力的重要性。   
人类认知能力主要通过对概念的识别、预测、确认、修改、完善。 对于机很难对于概念进行定义，可以通过类比人的认知概念对机器的认知进行定义: 
    * bAbi数据集,从认知能力出发，设计20种类型问题，类比定义机器阅读理解所需要的认知推理能力。  单句支撑问题、双句支撑问题、三句支撑问题、双参数关系、三参数关系、判断、计数、列举、否定、共指关系、聚合、聚合共指、时间推理、基本演绎、基本归纳、位置推理、大小/体积推理、目标动机推理、路径推理。  
    * 东京大学，从数据角度出发，将机器阅读理解所需要的认知能力进行定义。   
    ![tu](.\图片\东京大学老哥定义机器阅读理解需要的认知能力.png)   

1.3.2 测试方法: 
阅读理解测试可以粗略的分为: 1) 语言能力的测试; 2) 理解能力的测试;   
田师兄的PPT中的分类学
1.4 研究历程: 
1.4.1 数据: 

1.4.2 方法: 
1.4.2.1 基于符号工程的方法:  

1.4.2.1 基于分布式表示的深度学习方法: 
    * 基于构建: CNN\RNN\Transformer
    * 增强机制: 
      * Memory 机制[Gate]: 
        * Memory Network 
        * Recurrent entity network: 由多个动态记忆单元组成，每个单元学习表示输入中提到的实体 的 状态、属性
        * KG-MRC： 从文本中构建KB，然后用于MRC增强

      * Attention机制: 
        * Attention in CNN/RNN
        * Self-Attention in Transformer
      * Pre-Trained Model Representations
        * Bert 
        * XLNet
      * Fine-Tune add Knowledge 
          * k-BERT
          * 下游任务 加注意力约束，使用句法分析。
    * 模型介绍: 

1.5 研究挑战: 
    * 模型需要常识推理能力: 
    * 模型的可解释性: 
    * 模型针对特定数据、特定问题的解决能力:
    * 模型后其部署应用的实时性: Bert 参数量
    * 数据集设计:应该针对特定检测模型能力。  
    * 数据集设计: 数据中的偏置。  
    * 模型因果关系增强
