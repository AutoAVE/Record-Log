## 陈丹绮 博士论文： NEURAL READING COMPREHENSION AND BEYOND  [原文链接](https://stacks.stanford.edu/file/druid:gd576xb1833/thesis-augmented.pdf)   
总共7章，下面1-7分别代表，主要是两部分，阅读理解基础 以及  以及阅读理解应用     
* 建议阅读顺序-方法: 
  * 先懂得 陈丹绮是斯坦福曼宁的学生，2018年博士毕业，正好处在MRC蓬勃发展的几年，虽然文章不是很多，但是他在大的背景下还是做了贡献。  
  * 如果知道CoQA数据集，DrQA模型、SAR模型，那博士论文的一大半已经没有什么可以看的了。 
  * 如果先前有MRC基础，直接可以看，danqi对于MRC的定义,以及 MRC vs. QA部分，虽然不是很认同MRC vs.QA部分，但是我觉得她说的也有道理。  
  * danqi的博士论文的起点和终点都很高，总结了现在的发展，展望了未来的发展，仔细对照今年的文章很多其实她都有提到，当然这可能本来就是一个大的趋势，也只能这样发展，但是她提出来了。  
* 后期自己要再看注意点: 
    * 1977 Wendy Lehnert对于MRC的定义  
    * danqi对于 MRC的理解与定义: 偏向于NLU的综合性；期望于 解决更多的问题。  
    * danqi对于 MRC vs.QA的理解: 两者暂且可以认为  MRC是QA一部分，但是两个的终极目标不同,QA倾向于回答问题.  MRC倾向于 深度理解文章。  [当然我自己觉得也是这样，但我不觉得有明显的区别，在我看来，都只是一种暂且大家的做法、数据集的构建的一种形式而已，最终肯定是 NLU最重要,对于只智能体而言，肯定两者都很重要，甚至 MRC中的R可以不用显式体现，就像有的数据集没有文章，这些都不重要。  我自己好像也不能完全说清楚，确实是没有一个硬性的规范去区分，定义他们的区别，当然做实验时候，这些都没用，也不用考虑。 ]    
  * danqi将自己的工作划分为了基础和应用，不知道她是怎样考虑这件事情，在我看来CoQA 数据集，DrQA模型就是很普通的数据集和模型，但是当 提升到 MRC的应用角度，才发现danqi考虑的高度很高了。  
* DrQA项目地址: https://github.com/facebookresearch/DrQA  
#  danqi博士论文-博士生涯 总结概图:  
* google-scholar: https://scholar.google.com/citations?hl=zh-CN&user=sVR8ktkAAAAJ    

![学术主页](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00028_陈丹绮_谷歌学术主页截图.png)  
* 总共21篇文章，引用3876  

![danqi 博士论文概览](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00026_陈丹绮博士论文概览.png)



<!-- TOC -->

    - [陈丹绮 博士论文： NEURAL READING COMPREHENSION AND BEYOND  [原文链接](https://stacks.stanford.edu/file/druid:gd576xb1833/thesis-augmented.pdf)](#陈丹绮-博士论文-neural-reading-comprehension-and-beyond--原文链接httpsstacksstanfordedufiledruidgd576xb1833thesis-augmentedpdf)
- [danqi博士论文-博士生涯 总结概图:](#danqi博士论文-博士生涯-总结概图)
- [目录](#目录)
    - [**1.  简介**](#1--简介)
        - [1.1 **动机**  ---为什么要研究MRC? 出发点在哪里？](#11-动机-----为什么要研究mrc-出发点在哪里)
        - [1.2 **论文概述**  --- 博士论文的章节安排](#12-论文概述------博士论文的章节安排)
        - [1.3 **贡献**  --- 自己博士论文以及博士生涯的总结](#13-贡献------自己博士论文以及博士生涯的总结)
    - [**神经阅读理解: 基石**](#神经阅读理解-基石)
        - [2. 阅读理解综述:](#2-阅读理解综述)
            - [2.1 历史](#21-历史)
                - [2.1.1  早期系统](#211--早期系统)
                - [2.1.2 机器学习方法](#212-机器学习方法)
                - [2.1.3 深度学习方法](#213-深度学习方法)
            - [2.2 任务定义](#22-任务定义)
                - [2.2.1 问题形成](#221-问题形成)
                - [2.2.2 评价](#222-评价)
            - [2.3 阅读理解  和 问答系统    MRC vs. QA](#23-阅读理解--和-问答系统----mrc-vs-qa)
            - [2.4 数据集和模型](#24-数据集和模型)
        - [3. 神经阅读理解模型  [以前方法 vs 自己方法 SAR]](#3-神经阅读理解模型--以前方法-vs-自己方法-sar)
            - [3.1 过去的方法: 基于特征模型](#31-过去的方法-基于特征模型)
            - [3.2 神经方法: The Standord Attentive Reader  （SAR模型）](#32-神经方法-the-standord-attentive-reader--sar模型)
            - [3.3 实验结果:  SQuAD--CNN/Daily-Mail 数据集](#33-实验结果--squad--cnndaily-mail-数据集)
            - [3.4 SAR 模型可以进一步进行扩展:](#34-sar-模型可以进一步进行扩展)
        - [4. 阅读理解前景](#4-阅读理解前景)
            - [4.1 SQuAD 数据集解决了吗？ 现有模型的分析](#41-squad-数据集解决了吗-现有模型的分析)
            - [4.2 未来的工作: 数据集](#42-未来的工作-数据集)
            - [4.3 未来的工作: 模型](#43-未来的工作-模型)
                - [4.3.1 需求定义](#431-需求定义)
                - [4.3.2 框架和模型](#432-框架和模型)
            - [4.4 研究问题](#44-研究问题)
                - [4.4.1 如何衡量模型的进步？    [几篇分析的工作]](#441-如何衡量模型的进步----几篇分析的工作)
                - [4.4.2 表示与模型框架: 哪个更重要？](#442-表示与模型框架-哪个更重要)
                - [4.4.3 数据](#443-数据)
    - [**神经阅读理解: 应用**  [danqi这两方面的工作]](#神经阅读理解-应用--danqi这两方面的工作)
        - [5.开放域问答系统   DRQA模型](#5开放域问答系统---drqa模型)
            - [5.1 开放域问答系统历史](#51-开放域问答系统历史)
            - [5.2 Danqi--DRQA](#52-danqi--drqa)
            - [5.2.1 概述](#521-概述)
            - [5.2.2 文本抽取   Document Retriver 模块](#522-文本抽取---document-retriver-模块)
            - [5.2.3 文本阅读  Document Reader 模块](#523-文本阅读--document-reader-模块)
            - [5.2.4 距离监督](#524-距离监督)
        - [5.3 评价](#53-评价)
            - [5.3.1 问答系统数据集](#531-问答系统数据集)
            - [5.3.2 实施细节](#532-实施细节)
                - [5.3.2.1 Processing Wikipedia](#5321-processing-wikipedia)
                - [5.3.2.2 Distantly-supervised data](#5322-distantly-supervised-data)
            - [5.3.3 文本抽取表现   Document Retriever Performance](#533-文本抽取表现---document-retriever-performance)
            - [5.3.4 最终结果](#534-最终结果)
        - [5.4 未来工作](#54-未来工作)
        - [6.对话问答系统     CoQA数据集](#6对话问答系统-----coqa数据集)
            - [6.1 相关工作](#61-相关工作)
            - [6.2 CoQA数据集 介绍](#62-coqa数据集-介绍)
                - [6.2.1 任务定义  [数据集特点]](#621-任务定义--数据集特点)
                - [6.2.2 数据集收集](#622-数据集收集)
                - [6.2.3 数据集分析](#623-数据集分析)
            - [6.3 模型  ---baseline 以及自己尝试](#63-模型-----baseline-以及自己尝试)
                - [6.3.1 对话模型     Pointer-Generator-Network PGNet  --具体看图理解](#631-对话模型-----pointer-generator-network-pgnet----具体看图理解)
                - [6.3.2 阅读理解模型   SAR模型](#632-阅读理解模型---sar模型)
                - [6.3.3 一种混合模型](#633-一种混合模型)
            - [6.4 实验](#64-实验)
                - [6.4.1 设置](#641-设置)
                - [6.4.2 实验结果](#642-实验结果)
                - [6.4.3 错误分析](#643-错误分析)
            - [6.5 讨论](#65-讨论)
    - [7. 总结](#7-总结)

<!-- /TOC -->
# 目录   
## **1.  简介**  
### 1.1 **动机**  ---为什么要研究MRC? 出发点在哪里？    
* MRC是检测评估机器对于语言理解最合适的任务。 
  * NLU其他任务:  都是尝试在对 语言进行分析和理解 ， 但是阅读理解更像是一个综合，都研究了,阅读理解系统同时要检测系统的各个方面的能力。    
    * 词性标注POS  
    * 命名实体识别NER
    * 语法分析Syntactic Parsing 
    * 指代消解Corefereence Resolution 
    * 其他任务    
  * NLU 其他任务 与 MRC做法的联系 [图解，一个文本的理解涉及到好几个部分]       
  
![NLU-task-MRC](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00028_NLU_MRC-对比.png)

* MRC 的进展可以进一步推动其他任务的理解和进步: 
  * 开放域问答系统 
  * 对话问答系统  
  * 等等 ,总而言之,mrc做了其他很多任务也就做了。   



### 1.2 **论文概述**  --- 博士论文的章节安排    
* 总结阅读理解的历史    [时间节点图---1970、1999、]
  * 方法: 
  * 数据集
  * 研究热点  
* 定义阅读理解任务  
* 介绍自己在阅读理解任务上的贡献: SAR  
* 介绍自己在阅读理解应用层面的贡献: 
  * DRQA ---- 开放域问题系统方面  
    * 将 信息检索和阅读理解 结合
    * 从 Web 或者  百科 中找到通用问题的答案
  * CoQA ---- 对话问答系统方面   
    * 结合 对话和阅读理解，解决多轮对话的QA理解问题
    * 偏MRC，而不是注重对话系统，有大量QA问题和答案  


### 1.3 **贡献**  --- 自己博士论文以及博士生涯的总结    
* SAR模型  
* 阅读理解的分析  
* DrQA模型
* CoQA数据集

## **神经阅读理解: 基石**  
### 2. 阅读理解综述:   
#### 2.1 历史       
![历史记录](https://github.com/AutoAVE/Record-Logger/blob/master/图片/陈丹绮阅读理解数据集发展.png '历史')
##### 2.1.1  早期系统   
* 1970 年提出，MRC可以测试机器理解能力
* 1977年有一次发展: QUALM: 最早期的MRC基于脚本的系统，可以回答一定问题，但是很难泛化  
  * When a person understands a story, he can demonstrate his understanding by
answering questions about the story. Since questions can be devised to query any
aspect of text comprehension, the ability to answer questions is the strongest
possible demonstration of understanding. If a computer is said to understand a
story, we must demand of the computer the same demonstration of understanding
that we require of people. Until such demands are met, we have no way of
evaluating text understanding programs.   --Wendy Lehnert, 1977 [对于MRC定义]   
* 1999 年 Hirschman 提出: 60dev-60test数据集  3-6岁孩子智力  who, what, when, where and why questions  
  * 词干分析、语义类识别、、代词解析等  
* 2000 一些规则和词汇、语义对应 
##### 2.1.2 机器学习方法   
* 2013--2015  MCTest       f : (passage, question) −→ answer   
  * 基于手工特征: 句法依赖、共指消解、词向量。  
    * 缺点: 
      * 过度依赖 语义分析工具，专门训练，扩展性差。 
      * 很难找到和人类理解相同的特征 
      * 数据集太小  
  * MCTest  数据集    2013 
    * 660虚构故事,4个假设答案，1个正确 [和现在的RACE形式上基本一样，只是难度差别]    
    * 方法: 
      * 1) 不使用训练数据，滑动窗口法。 测量问题、答案和滑动窗口中单词的加权单词重叠、距离信息
      * 2) 将问答对转换为 文本蕴涵的系统
  * PROCESSBANK 数据集  2014
    * 200段落，585问题组成 
    * 生物过程中的二进制选择问题    
    * 要求理解过程中  实体和事件的关系  
  
##### 2.1.3 深度学习方法  
* 2015 CNN/Daily Mail 数据集 &  深度学习方法  
  * Cloze-style使得数据集大规模扩展  100W个数据示例  
  * Mask一个实体让模型选择一个实体回答问题，为了避免先验、世界知识干扰，使用了 匿名Entity@ 方法 
  * The Attentive Reader ---
    * 第一个 神经阅读理解系统 :  Attention-Based LSTM 
    * 证明神经阅读理解 性能远远好于 符号系统。     
* 2016 SAR模型 、 SQuAD数据集  
  * SAR模型 
    * 分析数据
    * sar模型
    * 证明: 与传统基于特征的分类器相比，神经网络模型能够更好的是被 词汇匹配和释义。 
  * SQuAD数据集[和陈丹绮无关，斯坦福大学]
    * 536 篇维基百科文章、107785问答对。 
    * 问答对---众包
    * span答案。  
    * 2018年未知: 2016特征: F1:51  --> 2018 Bert： F1: 91.8  人类: 91.2 
  * 神经阅读理解模型VS传统特征分类器优势: 
    * 不需要语法、语义特征，扩展性强
    * 特征方法: 特征稀疏[很难找到通用特征]、泛化性差，神经模型: 可以很大程度缓解稀疏性 
    * 不用人手工设计特征。
* 2017--2018 大量其他阅读理解数据集
  * 数量大
  * 数据来源多样化
  * 推理难度大

#### 2.2 任务定义  

##### 2.2.1 问题形成 
##### 2.2.2 评价 
* f : (p, q) −→ a  
* 根据answer不同形式，按问题可以分为: 
  * 完形填空:  问题中一个词语被 抠掉 MASK. 然后补全
    * 答案可以从预先设定的选项 或 整个词表中选出合适的词语或者 实体 --> acc评价    
      * 预先定义:  
      * 整个词表:  
      * 实体:     
      * Bert中分别怎么处理现在还是不清楚，后面应该会了解。  
  * 选择题 ---acc评价  --从预设答案中选择一个正确的答案
  *  抽取式: 从文章中选span作为答案  ---> EM\F1  
  *  问答式、自由文本 --- > BLEU\Meteor\ROUGH 
     *  一般对于每个问题都会设置 多个golden答案，计算指标时，选择最大的。 
 
#### 2.3 阅读理解  和 问答系统    MRC vs. QA
* 一般认为: RC是QA的一种，rc本质还是短文(有、无) 回答问题。形式、问题形成、方法、评价有很多共同点。   
* 但是: 两者最终目标上强调了不同的东西: 
  * QA: 目标: 建立能够自动回答人类提出问题的计算机系统，无论依靠何种资源。 
    * 资源可以是: 1）结构化知识库; 2) 非结构化文本  3)半结构化表达 4)其他模式。 
    * 终点: 1) 搜索识别相关资源; 2) 整合来自不同信息片段的答案;  3) 研究生活中人会遇到哪些问题。   
  * MRC：更强调文本理解，用回答问题来衡量 理解语言的能力。  所以: 回答问题需要对 所给的段落有深刻的理解。
    * 文本理解 和 世界理解的区别;  
  * 两者区别: 类似于人们在 搜索引擎上问的问题  和  阅读理解测试中  提出的问题。  
  * 宏观理解: 重点侧重分析文本中事实的简单描述，处理结合跨文档的信息冗余。 
  * 微观[MRC]: 侧重于 对于文本的深层次理解。  
#### 2.4 数据集和模型   
* 1):  大规模数据集使得训练神经模型成为可能，  同时也证明了模型相对于符号系统的竞争力。  
* 2):  模型总结数据集问题，进一步构建质量好的数据集。   

### 3. 神经阅读理解模型  [以前方法 vs 自己方法 SAR]    
#### 3.1 过去的方法: 基于特征模型   
* 陈丹绮 acl2016 --CNN/Daily Mail --基于特征的方法: 8中特征: 出现次数、单词距离、出现位置等  
* 手动涉及特征向量，然后使用传统机器学习分类器
* 手工设计的特征: 
  * 实体信息: 频率、位置、if iff a q-p word? 
  * 与 文章-问题的对齐特征: 语法匹配、贡献、距离等等

* 同期其他人使用 特征-机器学习方法 在 SQuAD、TriviaQA上做。 
#### 3.2 神经方法: The Standord Attentive Reader  （SAR模型）  
  *  SAR模型图  
   
![SAR模型图](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00001_SAR模型图.png 'SAR模型图')
* 模型简介: 
  * 关键点1: Passage的编码部分: 
    * 1) 与 问题question的对齐向量，通过相似度计算得到  
    * 2)加入和question的硬对齐 EM = 1/0   
    * 3) 加入实体类型 NER  
    * 4) 加入POS-TF等： 好多都是离线预先得到结果
  * 关键点2: passage 编码之前对齐，最后lstm之后进一步和 question计算相似度进行对齐(软对齐) 
  * 关键点3: 可以最后改变输出以及loss计算适应于 其他几种类型的MRC
#### 3.3 实验结果:  SQuAD--CNN/Daily-Mail 数据集    
* 数据集简介:  
![数据集](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00005_SAR实验数据集介绍_SQuAD_CNN-DM.png) 
* 1) CNN/Daily-Mail 数据集 ----Cloze_style test    

![CNN/DM实验结果](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00002_SAR实验结果1.png)  
* 2) SQuAD数据集-----抽取式  
  
![SQuAD实验结果](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00003_SAR实验结果2_SQuAD.png) 
* 实验结果分析: 
* SQuAD数据集 
  * 消融实验结果:  
![SQuAD消融实验](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00004_SAR消融结果_SQuAD.png)   
  * 结论: 
    * EM和软对齐对于最终的结果都有影响，而且两者互补，同时去掉，结果下降很大。   
    * 数据集中 Q -- P之间存在很多重叠而且对于最终答案影响很大。  
* 分析模型到底学到了什么？  ---CNN/Daily_Mail 数据集  
  * 采样100数据进行分类: 
  * ![CNN数据采样分类](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00006_CNN数据集采样100手工分类.png)
  * 说明:   
    * Partial clue： 问题和文章部分匹配  
    * Multi-sentences： 回答问题需要多句推理  
    * 数据中的错误: 共指错误+人类很难作对占比: 25%--> 数据集很大噪音。  

  * SAR按照类别的实验结果对照:   
  ![CNN/DM分类型实验结果](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00007_SAR在CNN数据集分类型结果.png)  
  * 说明：   
    *  ambiguous/hard and entity-linking-error 情形SAR和之前都很差。 符合预期;  
    *  EM太简单都很好; 
    *  Paraphrsing、Partial clue ： SAR远远好于feature-based
    *  Neural Network很强的捕获 语义匹配能力(释义、两个句子之间单词变化) 


#### 3.4 SAR 模型可以进一步进行扩展: 
  * glove  --> ELMO、Bert、character_embedding 
  * Attention： BiDAF_双向attention[question-to-passage attention，已经被证明没用，问句太短，lstm直接可以建模，不需要进一步和文章进行注意力对齐]、passage上self-attention[文章中实体自己关注对齐]  
  * ![attention机制](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00008_SAR模型扩展_注意力图示.png)
  * LSTM--> Transformer、QANet(多Conv层+self-attention+fc)、SRU等  
  * 训练目标: 
    * span预测   交叉熵 ---> F1： 使用强化学习reward等进行训练。  [后面可以想想细节]  
    * free-form text： 使用 sentence-level的损失函数进行训练。  
  * 数据增强: back-translation  
  
![SAR改进](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00009_SAR_改进点_SQuAD上被证明.png)

  


### 4. 阅读理解前景      
尽管SQuAD数据集来说已经分数很高了。 从实验结果分析来看，模型对于文章的理解还是仅仅停留在表面的文本信息上，对于文章的深入理解并没做到，甚至在文章末尾加一句就会迷惑模型。  SQuAD数据集任务相对于来说还算是比较简单，离推理差的很远。 

#### 4.1 SQuAD 数据集解决了吗？ 现有模型的分析     
* SQuAD数据集的局限性(结合 Jia&Liang2017的对抗性文章总结): 
  * SQuAD数据集中 问题和文章中词语重复度比较高，降低了难度。 对抗情况下性能下降特别快。   并没有理解文章。  
  * 数据集中问题一定是可以回答的单独span，基本全是基于 事实类型进行匹配的，why\how类型的问题少。    
  * 大部分问题都不要进行推理，13.6%需要进行推理，一部分使用  共指消解系统就可以解决。 

#### 4.2 未来的工作: 数据集    
* SQuAD之后新的数据集--  
1) 需要跨句、跨文档进行推理; 
2) 需要处理长文档
3) 需要生成答案 而不是 single span 
![SQuAD之后新的数据集](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00010_SQuAD之后新的数据集.png)
* TriviaQA： trivia and quiz-league websites 然后找到 网上的
  * 主旨: q-a对在没看见相应文章之间进行收集。 
  * 1) 95K Q-A pairs from trivia and quiz-league websites 
  * 2) 去网上找 textual evidence from Web search 或者 wikipedia中，其中找的是和 问题中实体提及相关的页面。   
  * 3) 最终650k p-q-a是triples
  * 意义: 1) 解决了问题依赖于文章  2) 构建数据集容易。 3)文档平均长度2895单词是SQuAD 20倍。  
  * 缺陷: 和 CNN\Daily-Mail[??体现在哪里]一样数据集构建是基于启发式的，并不能保证文章中真正包含了正确答案。  
* RACE: 12-18岁中国 初中、高中英语考试题。 ---> 难
  * 26% : 需要多句子reasoning  
* NarrativeQA[2018]：  基于wikipedia里面  书或者是电影  的摘要进行提问，内容比较复杂多样
  * Free-form text  | 众包者被鼓励用自己的话语作答
  * 两种设置: 
    * 1) 基于摘要: 类似于SQuAD 平均长度: 659 
    * 2) 基于全文: 62528 tokens 平均每个脚本: 很难，需要IR在长文档中进行定位;  
* SQuAD2.0 ： 增加不可回答问题类型  
  * 增加 53775 反例，从文章中来看是不可回答的，但是看起来文章中有一个相似的答案。  
  * 系统不仅要回答问题，而且要确定文章中什么时候是不可回答的。  
* HotpotQA： 多文档、多跳回答问题;  
  * 10段落--2 段是 evidence 
  * all wiki  
#### 4.3 未来的工作: 模型    

##### 4.3.1 需求定义  
##### 4.3.2 框架和模型    
* 模型结构方面: Structures and Modules
  * 加入语言学的特征，而不是自动去学习; 如: NER、POS等等；
    *  加入更多的linguistic feature，用tool就需要考虑tool的质量； 
    *  直接一起训练当作 隐变量，就要考虑怎么融入比较合理。 
  * 现有推理理解进行Module化、结构化，解决一个综合问题可以分解为许多子问题，然后解决每一个小的mdule，然后根据任务组合他们。 (NMN--现在已经有很多模型这样去尝试了)  
  * 陈丹绮观点: NMN现在仍然用于mrc比较难，因为语言太复杂，很难定义 modules,现在drop数据集上已经有很多工作在做这些了。   
* 不考虑模型结构: 
  * 速度和可扩展性: 快、长文档也支持  
  * 鲁棒性: 1) 加入对抗样本； 2) 迁移学习；  3) 多任务学习
  * 可解释性: 可用方法: 
    * 让模型学会抽取 支撑观点;  VCR数据集，补给你回答问题，还需要证据对。 
    * 学习生成依据，而不仅仅是抽取，生成 rationales. 
    * 收集人类在 阅读推断时候的证据。 做 train-reasoning 资源。  
#### 4.4 研究问题    
##### 4.4.1 如何衡量模型的进步？    [几篇分析的工作]
[Sugawara_2017] [Sugawara_2018] [Kaushik & Lipton 2018]
* 如何判断取得了真正的进步？  
* 如何判断一个基准上的进展可以扩展到 其他基准上面？  
* 如何判断这些系统和人类的差异性？   
* 解决办法: 
  * 使用 人参加的标准考试  来批评机器系统的理解能力
  * 将许多数据集集成一起测试模型的泛化能力---danqi_2019 EMNLP_Workshop 工作 
* 更重要: 理解现有数据集，描述他们质量和回答问题所需要的技能。  [东京大学--专门做MRC分析的]
  * 了解数据集需要什么？ 
  * 系统能做什么？ 系统不能做什么？  
  * 然后基于 这些属性进行 构建数据集 和 开发模型。  
##### 4.4.2 表示与模型框架: 哪个更重要？         
* SQuAD数据集上面 BiDAF--Bert做法: 
![模型比较](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00011_表示学习模型_复杂结构模型.png)

* 作者观点: Bert太粗糙，难以进行推理，以后模型是  bert-复杂模型

##### 4.4.3 数据  
* 数据增多是否可以提高系统性能？   
* 是否少量数据依赖预训练模型 提高系统性能？   
* 未来: 
  * 研究无监督学习 
  * 研究迁移学习
  * 研究 如何 低成本构建监督数据集



## **神经阅读理解: 应用**  [danqi这两方面的工作]
### 5.开放域问答系统   DRQA模型   
第一部分讲了 阅读理解任务以及近些年的发展，但是仍然不清楚: 阅读理解是不是仅仅是一项用来衡量 语言理解的一项任务，还是说 它也可以带来一些实际的应用。  所以第二部分，主要以 阅读理解的应用展开，然后进行阅读理解在应用层面的讨论。  

#### 5.1 开放域问答系统历史     
**open domain question answer**： 
  * 目标: 建立一个自动计算机系统，该系统可以基于 庞大的 1) 非结构化自然语言文本集; 2) 结构化的数据(ig：知识库); 3) 半结构化的数据(ig:表格); 4) 其他形式(图像、视频) ， 来回答人们提出的 任何问题。  
  * 作者希望通过研究 阅读理解技术  和  检索技术  的结合和应用，进一步提高 开放域问答系统的性能。     
* open domain question answer 历史: 
  * 1964：PROTOSYNTHEX system: 根据问题中的内容词进行查询，然后根据 频率加权项 和 问题重叠检索候选答案句，最后: 使用 依存分析得到答案
  * 1993: MURAX ： 使用浅层语言知识+IR 检索在线百科全书 回答一般知识问题；   
  * 1999: TREC比赛中增加QA: 1) IR--> 从问题检索 top n 相关的段落； 2) 窗口进行评分作答。  
  * 2007[freebase]-2008[dbpedia]--> KBQA系统: WebQuestions[2013]; SimpleQuestions[2015],KB-QA的主要方法是: 1) 基于语义分析 2)信息提取技术。 
  * 由于kbqa模式的据下你行，研究又回到 原始文本的设置。 
  * 文本资源、数据库资源等等；
  * 2002[AskMSR]--2010[DeepQA] -- 2015[YodaQA]
  * DeepQA是最具代表性的现代问答系统，使用  非结构信息 和 结构数据 共同生成候选答案。    
  * ![DeepQA](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00012_DeepQA系统_IBM_Waston.png)

* 从open domain QA的历史 引出 DrQA模型:   use Wikipedia as knowledge source.  
  * 主要介绍了其他工作使用 wikipedia做QA的经历。 
  * 介绍 DrQA拿wikipedia的文本文档作为唯一的资源，回答问题，强调阅读理解在 open domain QA的重要性。  
#### 5.2 Danqi--DRQA  
#### 5.2.1 概述    
* 作者提出的DRQA将Wikipedia作为唯一的知识源,并且只考虑文本内容。旨在建立一个知识问答系统，对于提出的任何百科问题，都可以从wikipedia中给出答案。  
* 选择 wikipedia作为唯一 knowledge resource 的原因: 
  * wikipedia在不断发展、壮大，包含了人们感兴趣的最新知识，而且这些知识容易被计算机处理、理解。 希望可以回答人们提出的任何常见的问题。  
  * 很多阅读理解数据集 都是建立在  wikipedia 之上的，所以可以利用这些已有的资源。 
  * wikipedia 文章干净、质量高、内容丰富、格式良好，所以是 回答 open domain QA的很好的资源。  
* DrQA模型及简介:   
  * MRC模型主要在open domain QA的 Doc Read 和 理解方面发挥作用。  
  * 系统包括两部分内容：1. 内容检索，找出相关的文章；2.阅读理解模型，从单个或多个文本提取答案。虽然使用Wikipedia收集数据，但不依赖于其内部的图结构。因此，这种方法可以适用与其他的文档，书记或者日报等内容。

![DrQA模型](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00013_DrQA_模型概述.png)  
#### 5.2.2 文本抽取   Document Retriver 模块  
* 参考使用QA中相关的检索系统。 文章-问题 使用 BoW向量的加权 TF-IDF;  
* N-gram 特征
* 最终剩余 5篇文章进行 MRC;  
#### 5.2.3 文本阅读  Document Reader 模块  
  * SAR阅读器; 
  * 多段落问题--> use the unnormalized exponential and take argmax over all considered paragraph spans for our final prediction  


#### 5.2.4 距离监督   
* 目前: IR-MRC已经有了，只剩下怎么训练了。  能想到最直接就是使用SQuAD去训练MRC模型，也是基于 wikipedia的，但是 SQuAD中 question 与 passage是很相关的，就直接从给的passage可以找到答案。  
* 为了解决这个问题，需要增加训练样本[再次使用之前的 IR模块]，建立一个远程监督训练样本，
* 形式如: f : (q, a) =⇒ (p, q, a) if p ∈ Document Retriever (q) and a appears in p
* 用来扩展和重新训练SQuAD在开放域上的模型。  
* Diatant Superversion[关系抽取],虽然又噪音，但是方便。 

### 5.3 评价  
#### 5.3.1 问答系统数据集   
* TREC：2015 
* WebQuestions:2013 
* WikiMovies: 2016 
* 说明:虽然这几个数据集不一定是在 wikipedia上面进行设计的，TREC是为 基于文本的问答设计; WebQuestions 和 WikiMovies主要是为 基于知识的问答而设计，希望将这些资源放在一个统一的框架内，然后测试 DrQA的性能，评价DrQA对于一般知识(general-knowledge)的回答能力;   
![DrQA模型数据集统计](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00013_DrQA_模型实验数据集统计.png)
* [有时有两个答案，DrQA设定一个对，就算对] 数据集统计2:    
![DrQA模型数据集统计2](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00015_DrQA_模型数据集统计2.png)
#### 5.3.2 实施细节   
##### 5.3.2.1 Processing Wikipedia  
只提取维基百科文本信息，时间戳2016-12-21，共包含5,075,182文章    
##### 5.3.2.2 Distantly-supervised data   
第一步，信息检索莫鲁埃对问题检索出对应的top5 文章，文章中与答案没有精确匹配的段落删除，段落小于25或长于1500的也滤除，段落中如果出现命名实体，移除其他不包含的段落；对于检索余下的段落，采用unigram 和 bigram对于检索到的每个页面中的每个剩余段落，我们采用unigram和bigram在问题与20个词窗口匹配答案，所有位置进行评分，保持重叠最多的前5个段落。 如果有零匹配就删除，然后添加DS训练对。
#### 5.3.3 文本抽取表现   Document Retriever Performance  
![DrQA——IR结果](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00016_DrQA_IR结果.png)  
* 说明: 正确答案 在 检索到的 top5 passage中的比例.   

#### 5.3.4 最终结果   
![最终试验结果](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00017_DrQA_几种版本最终实验结果.png)  
* 说明: DrQA三种不同的训练模式: 
  * SQuAD： 没有使用 远距离监督进行训练，直接使用 SQuAD数据集进行训练 Doc-Read 模型;  
  * FT(DS)：Doc-Read模型现在 SQuAD上面 pre-trained，然后再进行 远距离训练。   
  * MT(DS): Doc-Read模型在  SQuAD 和 远距离监督版本进行多任务学习。   
### 5.4 未来工作     
* Aggregating evidence from multiple paragraphs: 
  * DrQA使用了 argmax: j假定: 1) 每篇文章都包含答案，并且最终输出只有一个答案; 2) 不同文章之间答案的得分具有可比性。  
  * 改进: 
      * 1) 文章重要性重排序; 
      * 2) 远程监督数据进行去噪;    
* Making the DOCUMENT RETRIEVER trainable  
* Better DOCUMENT READER module  
* Better DOCUMENT READER module  
### 6.对话问答系统     CoQA数据集    
#### 6.1 相关工作    
* 任务性问答系统: 
* 闲聊型对话系统：
* CoQA: 对话是 基于 文本 和 段落的。  

#### 6.2 CoQA数据集 介绍   
* 数据集设计的目标，基础要求: 
  * i: 考虑人们对话时候 提出问题的自然性; 
  * ii: 保证问答中 答案的自然性(答案: free-form text， 但是答案来源可以是 基于 span的) 
  * iii: 确保问答系统的鲁棒性,可以适用于 各个领域(文学、新闻、科技、自然)  
##### 6.2.1 任务定义  [数据集特点]  
  * 答案: free-form text ,大程度源于 passage_span 
  * Given a passage P, a conversation consists of n turns,
  * each turn consists of (Qi, Ai, Ri), i = 1, . . . n, where Qi and Ai denote the question
  * the answer in the i-th turn, 
  * and Ri is the rationale which supports the answer Ai and must be **a single span of the passage**  
  * Ri 收集的目的: 帮助模型理解 答案是如何产生的，并改进模型。  但是 训练提供，评价 不提供；
  * Ri:  基本原理，文章中的片段。   
  * 主要是: 问题是一种会话的形式，当前会话问题的理解 依赖于 之前历史问题及其对应答案。   
##### 6.2.2 数据集收集    
* 众包过程: 2个注释者
  * 1个人 提问，另一个回答。 
  * 优点: 
    * 基于文章一个提问一个回答交流、语言是自然的。 
    * 当一个人 回答一个错误的问题 或者 提出一个 模糊的问题时，可以及时发现。  进行标记。  
    * 两个单独注释当遇到分歧时候可以通过单独窗口进行商定。  
  * 3.6 USD: for conversation collection； 
  * 4.5 USD: for collecting 3 additional answers for dev and test data.  
* Collection interface： 
  * 提问者 和 回答者  使用不同的接口。 
  * 提问者: 提出问题--> 要求: 尽量避免使用 与原文章精确匹配的词语，以增加词汇的多样性，尽可能对问题进行转述。 
  * 回答者: 1) rationales + answer. 2)answer 鼓励尽量与 文章词汇相同； 3) 可以对 文章内容复制、编辑，其中统计发现 78%发生编辑，如: 改变大小写、标点等。  
* Passage Selection： 
  * 借助其他已有数据集选择 7 个领域的文章: 
  * MCTest 2013 --children stories 
  * literature from Project Gutenberg
  * RACE 2017 -- 初中、高中英语考试  
  * CNN 2015 ---新闻  
  * wikipedia 文章 
  * AI2 Science Questions 2017 --  science 
  *  Writing Prompts dataset 2018 ---Reddit  
  *  使用NER选择文章中有 多个实体的文章，，并对 长文章进行了过滤、截取处理  
![CoQA数据集分布](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00018_CoQA数据集领域分布-统计.png)  
  * 说明: 域内 全用；   域外: 100 passage只是测试，不进行 training-dev     
  
**Collenting multiple answers**:   
* 给dev-test额外收集三个答案。    作用: 
  * 1. 我举得一方面是为了  评价的 伸缩性; 
  * 2. train时候并没有给出3个，而测试时候给出3个，可以根据上一个的答案，更好的帮助后面的问题 作答。  【这个在最后测试、评价时候起的影响应该怎样分析？  ？ 】  
  * 虽然3答案，但是鼓励 模型、人类的答案都尽可能与 原始答案一致，众包答案收集通过 先自己预测答案，然后和原始答案比较是否正确，每一步都这样做，尽可能让人类回答原始答案。  使用这种方法 后面验证人类回答问题的性能 F1--> 提高 5.4% 。 
 
  

##### 6.2.3 数据集分析   
* Comparison with SQUAD 2.0  
  * SQUAD2.0中不存在共指关系（he,him,she,it,they)；
  * SQUAD2.0几乎一半都是what问题。还有如did,was,is does,and都是几乎在SQUAD2.0中不存在的。
  * 而且问题的长度也有差别；
  * CoQA又11.1%和8.7%的yes和no的问答。    
* SQuAD  vs. CoQA:  1)P-Q-A 各自长度对比   2) 问题类型对比:  
![比较](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00018_CoQA数据集和SQuAD比较1.png)  
![比较](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00019_CoQA数据集和SQuAD比较2.png)    
* Conversation flow  
![会话流分析](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00020_CoQA中的会话流分析.png)  
  * X轴:随着会话进行  turn的进度条  
  * Y轴: 整个文章与 各个 chunk对应的长度关系。 
  * 块的高度表示对话的集中程度。频带宽度与从一圈到另一圈的频带转换频率成正比
  * 主要用来展现 会话过程中  谈论话题的 过渡性质 与 转变性质。   

* Linguistic phenomena    
![CoQA语言现象分析](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00021_CoQA数据集中的语言现象分布统计-举例.png) 





#### 6.3 模型  ---baseline 以及自己尝试  
##### 6.3.1 对话模型     Pointer-Generator-Network PGNet  --具体看图理解
![PGNet](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00022_Pointer-Generator-Network模型图.png)  
##### 6.3.2 阅读理解模型   SAR模型  
* 按照之前对于SAR模型的改进，修正SAR使用 

##### 6.3.3 一种混合模型    
SAR+PGNet

#### 6.4 实验  

##### 6.4.1 设置   
PGNet： Glove 
SAR： Fasttext 
##### 6.4.2 实验结果   
![实验结果](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00023_CoQA数据集_Baseline结果.png)  
![实验结果2](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00024_CoQA_Baseline_按照问题类型划分结果.png)  
##### 6.4.3 错误分析  
* 人类自己对于 unanswerable 和 answerable类型的定义都不是很清晰; 
* 问题类型结果分析 
* 共指影响分析      
* Importance of conversation history   
![会话历史影响](https://github.com/AutoAVE/Record-Logger/blob/master/图片/00025_CoQA_会话历史对于结果的影响.png)  
#### 6.5 讨论   
* 一些改进的方法: 比如 合理利用 training中的标注 Ritionles  


## 7. 总结    




